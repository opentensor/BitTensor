{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f263275-192d-4178-bbb7-b71a6c2f2bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c48cff0-fe87-455d-96b0-712f4a623db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79033c3c-ddce-4e90-9c38-0b34b88fc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14495806-3d4b-45a2-9e16-3179be394e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67b1b049-f4a8-4be8-9e81-65e806ea28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09602a0b-ddd4-4925-8d92-64cc842b1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": [
    "wallet = bittensor.wallet.Wallet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4cc0582-1f6b-4872-8f12-488972ce180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": [
    "dendrite = bittensor.dendrite.Dendrite( wallet = wallet )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac560c3-0931-472c-808b-41abcddbc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = bittensor.metagraph.Metagraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f9f98f-a4f9-48e2-95dd-9ca772db9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "\u001b[32m\u001b[1mConnected to network:\u001b[36mkusanagi\u001b[0m\u001b[32m\u001b[1m at endpoint:\u001b[36mfeynman.kusanagi.bittensor.com:9944\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 90.14it/s]\n"
     ]
    }
   ],
   "source": [
    "meta.load()\n",
    "meta.sync()\n",
    "meta.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "daa3a802-2ead-4ef6-8d44-3f0d5c991c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": [
    "sgmoe = SGMOERouter()\n",
    "sgmoe.sync_chain_state( meta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32e288e5-45be-4f69-b321-1c01251dd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sgmoe.forward_text (metagraph = meta, dendrite = dendrite, text = torch.tensor( [[1]]), query = torch.zeros([1, bittensor.__network_dim__]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "02669a88-d744-47d0-9ffc-42ecb0a0c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(request_sizes=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]), responses=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>), return_codes=tensor([1, 0, 2, 2, 2, 0, 2, 2, 2, 2]), weights=tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0083, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0082, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0084, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0080, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0084, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0083, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0084, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<ScatterBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "73dafb6e-60f1-4cb3-bbf9-e124768a5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "# Copyright © 2021 Yuma Rao\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated \n",
    "# documentation files (the “Software”), to deal in the Software without restriction, including without limitation \n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, \n",
    "# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of \n",
    "# the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n",
    "# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL \n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION \n",
    "# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER \n",
    "# DEALINGS IN THE SOFTWARE.\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from munch import Munch\n",
    "from typing import List, Tuple\n",
    "from types import SimpleNamespace\n",
    "import bittensor\n",
    "\n",
    "class SGMOERouter( torch.nn.Module ):\n",
    "    def __init__(self, config: Munch = None, query_dim = bittensor.__network_dim__, **kwargs):\n",
    "        super().__init__()\n",
    "        if config == None:\n",
    "            config = SGMOERouter.default_config();       \n",
    "        bittensor.config.Config.update_with_kwargs(config.router, kwargs) \n",
    "        self.config = config\n",
    "        self.query_dim = query_dim\n",
    "        \n",
    "        # Gating weights. Should match the metagraph.n\n",
    "        self.gates = torch.nn.ModuleList()\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    @staticmethod   \n",
    "    def default_config() -> Munch:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        SGMOERouter.add_args(parser) \n",
    "        config = bittensor.config.Config.to_config(parser); \n",
    "        SGMOERouter.check_config(config)\n",
    "        return config\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:    \n",
    "        parser.add_argument('--router.key_dim', default=100, type=int, help='Product keys dimension.')\n",
    "        parser.add_argument('--router.topk', default=20, type=int, help='Number of keys to select for each example.')\n",
    "        parser.add_argument('--router.stale_emit_filter', default=10000, type=int, help='Number of blocks before a neuron is filtered without a recent emit')\n",
    "        return parser\n",
    "\n",
    "    @staticmethod\n",
    "    def check_config(config):   \n",
    "        return config\n",
    "\n",
    "    def sync_chain_state( self, metagraph: 'bittensor.metagraph.Metagraph' ):\n",
    "        r\"\"\" Creates new parameters based on metagraph size.\n",
    "\n",
    "            Args:\n",
    "                metagraph (:obj: `bittensor.metagraph.Metagraph'`, `required`):\n",
    "                    bittensor metagraph object. Used to pull network endpoint info.\n",
    "        \"\"\"\n",
    "        # Add new gates for each uid.\n",
    "        for uid in metagraph.uids.tolist():\n",
    "            self.gate_for_uid = torch.nn.Linear( self.query_dim, 1, bias=True)\n",
    "            self.gates.append( self.gate_for_uid )\n",
    "\n",
    "    def _route(self, metagraph: 'bittensor.metagraph.Metagraph', dendrite: 'bittensor.dendrite.Dendrite', inputs: torch.FloatTensor, query: torch.FloatTensor, modality: bittensor.proto.Modality) -> SimpleNamespace:\n",
    "        r\"\"\" Routes inputs using context and metagraph state.\n",
    "\n",
    "            Args:\n",
    "\n",
    "                metagraph (:obj: `bittensor.metagraph.Metagraph`, `required`):\n",
    "                    bittensor metagraph object. Used to pull network endpoint info.\n",
    "\n",
    "                dendrite (:obj: `bittensor.dendrite.Dendrite`, `required`):\n",
    "                    bittensor dendrite object. User to make queries into the network.\n",
    "\n",
    "                inputs (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, *-1*)`, `required`): \n",
    "                    tensors inputs to distribute to neurons using context.\n",
    "                \n",
    "                query (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, query_dimension)`, `required`): \n",
    "                    Context tensor used to select which neurons query for each example.\n",
    "\n",
    "                modality (:obj:`bittensor.proto.Modality` of shape :obj:`(1)`, `required`):\n",
    "                    Bittensor forward modality type. Enum in [TEXT, IMAGE, TENSOR]\n",
    "\n",
    "            Returns:\n",
    "                output = SimpleNamespace {\n",
    "                    responses (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, bittensor.__network_dim__)`, `required`): \n",
    "                        Joined responses from each queried neuron.\n",
    "\n",
    "                    weights (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, metagraph.state.n)`, `required`): \n",
    "                        weights for each neuron per example.\n",
    "\n",
    "                    requests_sizes (:obj:`torch.LongTensor` of shape :obj:`(metagraph.state.n)`, `required`): \n",
    "                        number of requests sent to each uid in this batch.\n",
    "\n",
    "                    return_codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_neurons]`, `required`):\n",
    "                        dendrite call return codes.\n",
    "                }\n",
    "        \"\"\"\n",
    "        output = SimpleNamespace ()\n",
    "\n",
    "        # For ease of use.\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # Filter uids.\n",
    "        # all_uids: (torch.LongTensor): unique keys for each peer neuron.\n",
    "        # all_uids.shape = [metagraph.n]\n",
    "        all_uids = metagraph.uids # Returns a list of neuron uids.\n",
    "\n",
    "        # filtered_uids: (torch.LongTensor): keys filtered by emit.\n",
    "        # all_uids.shape = [metagraph.n]\n",
    "        current_block = metagraph.block\n",
    "        lastemit = metagraph.lastemit\n",
    "        staleness = (current_block - lastemit)\n",
    "        filtered_uids = all_uids[torch.where(staleness < 10000)] \n",
    "        n_uids = torch.numel(filtered_uids)\n",
    "        # print (filtered_uids)\n",
    "        #print (n_uids)\n",
    "\n",
    "        # Get weights for uids.\n",
    "        weights =  torch.cat( [ sgmoe.gates[ uid ](query) for uid in filtered_uids.tolist() ], axis = 1)\n",
    "        #print( weights)\n",
    "\n",
    "        # Mean of filtered scores across batch dimension.\n",
    "        # filtered_weights_mean = [ n_filtered ]\n",
    "        filtered_mean_weights = torch.mean(weights, axis = 0)\n",
    "\n",
    "        # Indicies for the filtered uids with the largest mean batch score.\n",
    "        # topk_indices = [self.config.topk]\n",
    "        real_topk = min( n_uids, 10 )\n",
    "        topk_weights, topk_indices = filtered_mean_normalized_weights.topk(real_topk, dim=0) \n",
    "        print (topk_weights)\n",
    "        \n",
    "        # real uids of filtered topk\n",
    "        real_filtered_topk_uids = filtered_uids[ topk_indices ]\n",
    "        print (real_filtered_topk_uids)\n",
    "        \n",
    "        # neurons: List[bittensor.proto.Neuron]: endpoint information for filtered uids.\n",
    "        neurons = []\n",
    "        for filtered_uid in real_filtered_topk_uids.tolist():\n",
    "            neurons.append( metagraph.neuron_endpoints[ filtered_uid ] )\n",
    "\n",
    "        # Request for filtered topk values.\n",
    "        requests = [ inputs for _ in range( len(neurons) )]\n",
    "        responses, retops = dendrite.forward_text(neurons, requests)\n",
    "\n",
    "         # Gate responses with weights.\n",
    "        # weighted_responses = real_topk * [ batch_size, sequence_dim, __network_dim__]\n",
    "        weighted_responses = []\n",
    "        for idx, (resp, join_weight) in enumerate(list(zip(responses, topk_weights))):\n",
    "            weighted_responses = resp * join_weight\n",
    "\n",
    "        # Stich responses together along last dimension\n",
    "        # stitched_responses = [ batch_size, sequence_dim, __network_dim__]\n",
    "        stitched_responses = torch.mean( weighted_responses, axis = 1 )\n",
    "\n",
    "        # Response is just the stitchec responses\n",
    "        output.responses = stitched_responses\n",
    "        output.weights = torch.scatter( torch.zeros( (metagraph.n), dtype = torch.float32), 0, real_filtered_topk_uids, topk_weights )\n",
    "        output.request_sizes = torch.scatter( torch.zeros( (metagraph.n), dtype = torch.float32), 0, real_filtered_topk_uids, batch_size )\n",
    "        output.return_codes = retops\n",
    "        return output\n",
    "            \n",
    "    def forward_image(self, metagraph: 'bittensor.metagraph.Metagraph', dendrite: 'bittensor.dendrite.Dendrite', images: torch.FloatTensor, query: torch.FloatTensor) -> SimpleNamespace:\n",
    "        r\"\"\" Forwards images to connected neurons using the passed context to learn connectivity.\n",
    "\n",
    "            Args:\n",
    "                metagraph (:obj: `bittensor.metagraph.Metagraph`, `required`):\n",
    "                    bittensor metagraph object. Used to pull network endpoint info.\n",
    "\n",
    "                dendrite (:obj: `bittensor.dendrite.Dendrite`, `required`):\n",
    "                    bittensor dendrite object. User to make queries into the network.\n",
    "\n",
    "                images (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, channels, rows, cols)`, `required`): \n",
    "                    Image tensors to forward.\n",
    "                \n",
    "                query (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, context_dim)`, `required`): \n",
    "                    query tensor used to select which neurons query for each example.\n",
    "            \n",
    "            Returns:\n",
    "                SimpleNamespace {\n",
    "                    responses (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, bittensor.__network_dim__)`, `required`): \n",
    "                        Joined responses from each queried neuron.\n",
    "\n",
    "                    weights (:obj:`torch.FloatTensor` of shape :obj:`(metagraph.state.n)`, `optional`): \n",
    "                        weights for each neuron per example.\n",
    "\n",
    "                    requests_sizes (:obj:`torch.LongTensor` of shape :obj:`(metagraph.state.n)`, `optional`): \n",
    "                        number of requests sent to each uid in this batch.\n",
    "\n",
    "                    return_codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_neurons]`, `required`):\n",
    "                        dendrite call return codes.\n",
    "                }\n",
    "        \"\"\"\n",
    "        return self._route(metagraph, dendrite, images, query, bittensor.proto.Modality.IMAGE)\n",
    "\n",
    "    def forward_text(self, metagraph: 'bittensor.metagraph.Metagraph', dendrite: 'bittensor.dendrite.Dendrite', text: torch.LongTensor, query: torch.FloatTensor) -> SimpleNamespace:\n",
    "        r\"\"\" Forwards text to connected neurons using the passed context to learn connectivity.\n",
    "\n",
    "            Args:\n",
    "                metagraph (:obj: `bittensor.metagraph.Metagraph`, `required`):\n",
    "                    bittensor metagraph object. Used to pull network endpoint info.\n",
    "\n",
    "                dendrite (:obj: `bittensor.dendrite.Dendrite`, `required`):\n",
    "                    bittensor dendrite object. User to make queries into the network.\n",
    "\n",
    "                text (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_dim)`, `required`): \n",
    "                    tensor of tokenized sentences.\n",
    "                \n",
    "                query (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, query_dim)`, `required`): \n",
    "                    Context tensor used to select which neurons query for each example.\n",
    "            \n",
    "            Returns:\n",
    "                SimpleNamespace {\n",
    "                    responses (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, bittensor.__network_dim__)`, `required`): \n",
    "                        Joined responses from each queried neuron.\n",
    "\n",
    "                    weights (:obj:`torch.FloatTensor` of shape :obj:`( metagraph.state.n )`, `optional`): \n",
    "                        weights for each neuron per example.\n",
    "\n",
    "                    requests_sizes (:obj:`torch.LongTensor` of shape :obj:`(metagraph.state.n)`, `optional`): \n",
    "                        number of requests sent to each uid in this batch.\n",
    "\n",
    "                    return_codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_neurons]`, `required`):\n",
    "                        dendrite call return codes.\n",
    "                }\n",
    "                \n",
    "        \"\"\"\n",
    "        return self._route( metagraph, dendrite, text, query, bittensor.proto.Modality.TEXT)\n",
    "\n",
    "\n",
    "    def forward_tensor(self, metagraph: 'bittensor.metagraph.Metagraph', dendrite: 'bittensor.dendrite.Dendrite', tensors: torch.FloatTensor, query: torch.FloatTensor) -> SimpleNamespace:\n",
    "        r\"\"\" Forwards tensors to connected neurons using the passed context to learn connectivity.\n",
    "\n",
    "            Args:\n",
    "                metagraph (:obj: `bittensor.metagraph.Metagraph`, `required`):\n",
    "                    bittensor metagraph object. Used to pull network endpoint info.\n",
    "\n",
    "                dendrite (:obj: `bittensor.dendrite.Dendrite`, `required`):\n",
    "                    bittensor dendrite object. User to make queries into the network.\n",
    "\n",
    "                tensors (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, bittensor.__network_dim__)`, `required`): \n",
    "                    tensors sent to connected neurons.\n",
    "                \n",
    "                query (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, query_dim)`, `required`): \n",
    "                    Query tensor used to select which neurons query for each example.\n",
    "            \n",
    "            Returns:\n",
    "                SimpleNamespace {\n",
    "                    responses (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_dim, bittensor.__network_dim__)`, `required`): \n",
    "                        Joined responses from each queried neuron.\n",
    "\n",
    "                    weights (:obj:`torch.FloatTensor` of shape :obj:`(metagraph.state.n)`, `optional`): \n",
    "                        weights for each neuron per example.\n",
    "\n",
    "                    requests_sizes (:obj:`torch.LongTensor` of shape :obj:`(metagraph.state.n)`, `optional`): \n",
    "                        number of requests sent to each uid in this batch.\n",
    "\n",
    "                    return_codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_neurons]`, `required`):\n",
    "                        dendrite call return codes.\n",
    "                }\n",
    "        \"\"\"\n",
    "        return self._route( metagraph, dendrite, tensors, query, bittensor.proto.Modality.IMAGE )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c68cf-cb6c-4ee3-9eef-cc3ed3228ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
